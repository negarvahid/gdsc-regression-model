{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression\n",
    "\n",
    "[**Lecture Slides**](https://docs.google.com/presentation/d/11Rz5rinLF_At1DOSeJLQFoumJbsBTnHoyaXVQx2cswQ/edit?usp=sharing)\n",
    "\n",
    "This lecture, we are going to use linear regression to predict the happiness of countries.\n",
    "\n",
    "**Learning goals:**\n",
    "\n",
    "- solve a regression problem\n",
    "- fit a linear regression model\n",
    "- predict using a linear regression model\n",
    "- visualize the predictions of a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "The United Nations (UN) thinks that machine learning can save the world, and has asked us to _predict_ the happiness of countries. üåç That's quite a task! Luckily, we have some help: a dataset mapping life expectancy to a happiness score. All we have to do is apply linear regression to predict this score, and we should have a model able to predict national happiness! ‚ò∫Ô∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Munging\n",
    "\n",
    "As always, let's load the `.csv` dataset and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('world_happiness_report.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of columns. We're only interested in _life expectancy_ (our features) and _happiness score_ (our labels). Weird! The values of life expectancy are between 0 & 1! Most other columns also have surprising ranges. This is because the data is _normalized_, meaning it is reshaped to fit a more convenient interval. This is common data preprocessing, so always keep an eye on your summary statistics when using a new dataset! In our case, this won't affect our linear regression model, so we can use these values directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['Health..Life.Expectancy.']\n",
    "labels = df['Happiness.Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two pandas `Series` for our features and labels. sklearn gets a bit confused when the features data is a _vector_ , since the usual scenario is to have _many_ features stored in a matrix. In our case we only have one feature, the life expectancy. So let's force it into a matrix using [`np.reshape()`](https://numpy.org/devdocs/reference/generated/numpy.reshape.html).\n",
    "\n",
    "You can see in the documentation that we can use the argument `-1` to refer to the existing dimension. Since we are taking a ($N$) dimensional vector and we want to transform it in a ($N \\times 1$) dimensional matrix, we can do it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.values.reshape(-1, 1)\n",
    "y = labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also renamed our features and labels to the convention when fitting supervised learning models: the mathematical notation for our feature matrix $X$ and our label vector $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train our linear regression model! sklearn does all the gradient descent optimization magic for us. You might remember from the slides that this includes:\n",
    "1. randomly initializing the model parameters $\\theta$\n",
    "2. estimating the cost function for that initial model $J$\n",
    "3. calculating the derivative of the cost function at that point $\\frac{dJ}{d\\theta}$\n",
    "4. taking a step proportional to the negative of that derivative $-\\alpha \\frac{dJ}{d\\theta}$\n",
    "5. going back to step 2 until the cost function has reached a minimum $\\frac{dJ}{d\\theta} \\approx 0$\n",
    "\n",
    "Thankfully we don't have to do it ourselves! Instead, sklearn offers the same api as most other learning algorithms: `.fit()` on the data, then `.predict()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... that's it üòÖ? Remember that the equation for this linear model with one feature is :\n",
    "\n",
    "$$ h_{\\theta}(\\textbf{x}) = \\theta_{0} + \\theta_{1} x$$\n",
    "\n",
    "So we can check our optimized model parameters. in sklearn, the intercept $\\theta_{0}$ is separated from all the other model parameters, the coefficients $\\theta_{i}$.\n",
    "\n",
    "Let's look at the intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = reg.intercept_\n",
    "theta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our only coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1 = reg.coef_[0]\n",
    "theta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our hypothesis was optimized through gradient descent to :\n",
    "\n",
    "$$ h_{\\theta}(\\textbf{x}) = 3.297 + 3.731 x$$\n",
    "\n",
    "Since we only have _one_ feature, we can plot this model on a 2D graph. On the x-axis, life expectancy, on the y-axis, happiness score. The data is represented as points, the model as a green line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "def add_line(ax, theta_0, theta_1):\n",
    "    x_vals = np.array(ax.get_xlim())\n",
    "    y_vals = theta_0 + x_vals * theta_1\n",
    "    ax.plot(x_vals, y_vals, linewidth=2, color='g')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(X, y)\n",
    "add_line(ax, theta_0, theta_1)\n",
    "\n",
    "ax.set_xlabel('Life Expectancy (norm)')\n",
    "ax.set_ylabel('Happiness Score (norm)')\n",
    "ax.set_title('Linear Regression of National Happiness');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, life expectancy really matters for a country's happiness! This kind of insights is simple, but has a high impact: modern medecine clearly improves humans' standards of living. Being free from illnesses and child mortality is a major factor in our happiness. Thanks to all the hard working doctors out there! üôá‚Äç‚ôÇÔ∏è\n",
    "\n",
    "‚ÑπÔ∏èThis kind of graph is easy to read and interpret, but often infeasible due to the high number of features $\\textbf{x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UN likes our regression model, and has an urgent task for us: the Kingdom of Narnia has asked to join the UN, and they can only approve their membership if they know their happiness score! The leader of Narnia, Aslan ü¶Å, unfortunately doesn't have this data... but he has carried out a census recently and measured the life expectancy of the Narnian population. Let's _predict_ the nation's happiness using our linear regression model. üîÆ\n",
    "\n",
    "The Narnian life expectancy is of 0.777 (in normalized units). sklearn's `.predict()` method expects a matrix again, so let's create a ($1 \\times 1$) `ndarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_narnia = 0.777\n",
    "x_narnia = np.asarray(0.7).reshape(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then predict the Narnian happiness using the linear regression model that we trained earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_narnia = reg.predict(x_narnia)\n",
    "y_narnia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narnians are 5.909 happy on average! Except \"happiness score\" is a strange unit, so we have no idea what that means... Instead, we can plot Narnia on our National Happiness graph to compare it to other countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(X, y)\n",
    "add_line(ax, theta_0, theta_1)\n",
    "ax.plot(x_narnia, y_narnia, marker='x', markersize=20, markeredgewidth=4, color='r')\n",
    "\n",
    "ax.set_xlabel('Life Expectancy (norm)')\n",
    "ax.set_ylabel('Happiness Score (norm)')\n",
    "ax.set_title('Linear Regression of National Happiness');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are now much more obvious: Narnia is a pretty happy nation üòÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-variate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to our help, the UN was able to accept Narnia's membership application, but are now facing another issue. The International Monetary Fund (IMF) is not very impressed by our \"happiness model\". They believe that national economy must surely be a factor in a country's happiness... and our model completely disregards it! üí∏ They suggest that we could make our prediction of _happiness score_ even better by adding the `GDP per capita` feature.\n",
    "\n",
    "Let's re-select our features to include the oddly named `Economy..GDP.per.Capita` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['Health..Life.Expectancy.', 'Economy..GDP.per.Capita.']]\n",
    "labels = df['Happiness.Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, no need to `.reshape()` our features, since they are already in matrix form! We don't even have to convert anything to NumPy `ndarrays`, since sklearn also works with `DataFrame`s out of the box. We'll still follow the mathematical naming convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like univariate linear regression, we first have to `.fit()` our model before we can `.predict()` with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X, y)\n",
    "theta_0 = reg.intercept_\n",
    "theta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our intercept, $\\theta_{0}$ is slightly different! This means that the addition of a new feature has changed how the hypothesis, $h_{\\theta}$, was fit to the data. i.e It looks like our model has improved üí™. Let's look at the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_i = reg.coef_\n",
    "theta_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have two features, life expectancy & GDP per capita , we now have two coefficients: $\\theta_{1}$ & $\\theta_{2}$. This means that our gradient descent optimized hypothesis now looks like this:\n",
    "\n",
    "$$ h_{\\theta}(\\textbf{x}) = 3.069 + 1.600 x_{1} + 1.424 x_{2}$$\n",
    "\n",
    "Where $x_{1}$ is our first feature, the life expectancy, and $x_{2}$ is our second feature, the GDP per capita. \n",
    "\n",
    "When referring to the linear regression model parameters in a multi-variate setting, we often represent all of them together in a vector:\n",
    "\n",
    "$$\\boldsymbol{\\theta} = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\theta_{2}  \\end{bmatrix} = \\begin{bmatrix} 3.069 \\\\ 1.600 \\\\ 1.424  \\end{bmatrix} $$\n",
    "\n",
    "Let's do that with our theta values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.append([theta_0], theta_i)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It's hard to imagine what the model looks like with only with those three parameters... but since we are _only_ using two features, we can still visualize it in a 3 dimensional graph! On the x-axis, the life expectancy, on the y-axis, the GDP per capita, and on the z-axis, the happiness score. The data is still represented as blue points, and the model's predictions now form a two dimensional _plane_ , defined by the equation for $h_{\\theta}$ above, and represented by the green grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def create_mesh(theta):\n",
    "    x_values = np.linspace(0, 1, 50)\n",
    "    y_values = np.linspace(0, 2, 50)\n",
    "    x_mesh, y_mesh = np.meshgrid(x_values, y_values)\n",
    "    z_mesh = theta[0] + theta[1] * x_mesh + theta[2] * y_mesh\n",
    "    return x_mesh, y_mesh, z_mesh\n",
    "    \n",
    "x_mesh, y_mesh, z_mesh = create_mesh(theta)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter3D(X.iloc[:, 0], X.iloc[:, 1], y)\n",
    "ax.plot_wireframe(x_mesh, y_mesh, z_mesh, rstride=5, cstride=5, alpha=0.4, color='g')\n",
    "ax.set_title('Multi-variate Linear Regression of National Happiness')\n",
    "ax.set_xlabel('Life Expectancy (norm)')\n",
    "ax.set_ylabel('GDP per Capita (norm)')\n",
    "ax.set_zlabel('Happiness Score');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a good looking model üòè Looks like the IMF was right! Take some time to think about what is represented, and how is in common with the 2D graph. Can you tell how our features _correlate_ with the label?\n",
    "\n",
    "üß†üß† How is this model limited? Can you think of ways to improve its prediction accuracy?\n",
    "\n",
    "‚ÑπÔ∏è Whilst this is a perfectly fine example of a 3D graph, there are other python librairies better equipped for this kind of advanced visualization, e.g [plotly express](https://plotly.com/python/3d-scatter-plots/#3d-scatter-plot-with-plotly-express).\n",
    "\n",
    "üß† The kingdom of Hyrule has life expectancy = 0.666, and GDP per capita of 0.888. How would you _graphically_ read the predicted happiness score for this nation?\n",
    "\n",
    "üí™ Now try to use your trained linear regression model to predict the happiness score of Hyrule quantitatively. Save the prediction as a variable called `y_hyrule` in the cell below.\n",
    "Pro-tip: think of the _shape_ that the sklearn method expects as argument. You can find an example in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "life_expectancy = 0.666\n",
    "gdp_per_capita = 0.888\n",
    "\n",
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "def test_predict():\n",
    "    print(y_hyrule % 0.001)\n",
    "    assert math.isclose(y_hyrule % 0.001, 0.00072019, rel_tol=1e-04), f'Incorrect value of y_hyrule, try again!'\n",
    "    print('Success! üéâ')\n",
    "    return y_hyrule\n",
    "\n",
    "test_predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "\n",
    "Today, we learned about our first supervised learning algorithm: **linear regression**. First, we re-stated the regression task with **mathematical notation**. We then discovered the **cost function**, which measures a model's errors across a dataset. We walked through an example of the **mean squared error cost function** on a toy problem about cats and internet procrastination. üò∏ This lead us to defining **linear models** qualitatively and mathematically. We had all the tools to explain **gradient descent**, an iterative numerical method for **minimizing** the cost function. We understood that it's this optimization procedure which allows the model to **learn** the best hypothesis to predict data accurately. Finally, we put this to **practice** by fitting univariate and multivariate sklearn `LinearRegression` models to the World Happiness Report dataset.\n",
    "\n",
    "# Resources \n",
    "\n",
    "## Core Resources\n",
    "\n",
    "- [Machine Learning on Coursera - Linear Regression](https://www.coursera.org/lecture/machine-learning/model-representation-db3jS)  \n",
    "Andrew Ng's always excellent course is particularly insightful for this section on Linear Regression.\n",
    "- [Gradient descent visualization](https://github.com/Shathra/gradient-descent-demonstration)  \n",
    "A simple and clear visualization of gradient descent for finding function minima.\n",
    "- [World happiness report - kaggle dataset](https://www.kaggle.com/unsdsn/world-happiness)  \n",
    "The dataset used in this notebook.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Gradient descent derivation](https://mccormickml.com/2014/03/04/gradient-descent-derivation/)  \n",
    "A mathematical derivation of gradient descent with accompanying graphs.\n",
    "- [3D gradient descent visualization](https://xavierbourretsicotte.github.io/animation_ridge.html)  \n",
    "A more involved visualization of gradient descent in 3D.\n",
    "- [Siraj - how to do linear regression using gradient descent](https://youtu.be/XdM6ER7zTLk)  \n",
    "Code-along video from Siraj on building linear regression optimization from scratch.\n",
    "- [Karpathy's loss function tumblr](https://lossfunctions.tumblr.com/)  \n",
    "A curated collection of the world's most beautiful or strange loss functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
